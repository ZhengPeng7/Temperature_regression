{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use dataset <u>__7__</u> as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading \ttrain and validation set \tfrom subject 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 .csv\n",
      "Reading \ttest_48 set \t\t\tfrom subject 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 .csv\n",
      "\t\tpath\tlabel\tK\n",
      "train:\t\t1047750\t1047750\t1047750\n",
      "val:\t\t500\t500\t500\n",
      "test:\t\t347700\t347700\t347700\n",
      "test_48:\t195\t195\t195\n"
     ]
    }
   ],
   "source": [
    "from inception_v4 import InceptionV4\n",
    "from keras.applications.densenet import DenseNet201\n",
    "from keras.layers import (GlobalAveragePooling2D, Dense, Conv2D, Flatten,\n",
    "                          Dropout, concatenate, Input, Activation, normalization, GlobalAveragePooling1D, Conv1D)\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import Model, load_model, model_from_json, Input\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import SaveModelOnMAE_tail, generate_generator, read_image_and_K_from_dir, read_48_points\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "random.seed(7)\n",
    "net = 'DenseNet201'\n",
    "with_K = 'tail'\n",
    "idx_test = [17, 13, 20, 21]\n",
    "\n",
    "\n",
    "# Dataset\n",
    "dir_path = './data/csv_files'\n",
    "train_paths, train_labels, train_K, test_paths, test_labels, test_K = read_image_and_K_from_dir(dir_path, idx_test=idx_test)\n",
    "\n",
    "test_paths_48, test_labels_48, test_K_48 = read_48_points(\n",
    "    dir_path='./data/csv_files', dir_points='./data/48_points', idx_test=idx_test\n",
    ")\n",
    "test_paths_48, test_labels_48, test_K_48 = np.squeeze(test_paths_48).tolist(), np.squeeze(test_labels_48).tolist(), np.squeeze(test_K_48).tolist()\n",
    "\n",
    "indexes_rand = list(range(len(train_K)))\n",
    "random.shuffle(indexes_rand)\n",
    "train_paths, train_labels, train_K = np.asarray(train_paths)[indexes_rand].tolist(), np.asarray(train_labels)[indexes_rand].tolist(), np.asarray(train_K)[indexes_rand].tolist()\n",
    "\n",
    "idx_validate = np.loadtxt('./data/validation_selection_index.txt').astype(int)\n",
    "validate_paths, validate_labels, validate_K = np.asarray(test_paths)[idx_validate].tolist(), np.asarray(test_labels)[idx_validate].tolist(), np.asarray(test_K)[idx_validate].tolist()\n",
    "\n",
    "indexes_rand = list(range(len(test_K)))\n",
    "random.shuffle(indexes_rand)\n",
    "test_paths, test_labels, test_K = np.asarray(test_paths)[indexes_rand].tolist(), np.asarray(test_labels)[indexes_rand].tolist(), np.asarray(test_K)[indexes_rand].tolist()\n",
    "\n",
    "print('\\t\\tpath\\tlabel\\tK')\n",
    "print('train:\\t\\t{}\\t{}\\t{}'.format(len(train_paths), len(train_labels), len(train_K)))\n",
    "print('val:\\t\\t{}\\t{}\\t{}'.format(len(validate_paths), len(validate_labels), len(validate_K)))\n",
    "print('test:\\t\\t{}\\t{}\\t{}'.format(len(test_paths), len(test_labels), len(test_K)))\n",
    "print('test_48:\\t{}\\t{}\\t{}'.format(len(test_paths_48), len(test_labels_48), len(test_K_48)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./acceptable_weights/weights_tail_K/DenseNet201_tail_K_MAE0.324.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade keras 2.1.5 -> 2.2.4\n",
    "base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "flat = GlobalAveragePooling2D()(base_model.output)\n",
    "flat_len = 1\n",
    "for i in range(1, len(flat.shape)):\n",
    "    flat_len *= int(flat.shape[i])\n",
    "K_len = min(flat_len // 1, 2048)\n",
    "input_K = Input((K_len, 1))\n",
    "# K_flow = Activation(activation='linear')(input_K)\n",
    "K_flow = Conv1D(K_len//3, 3, activation='relu')(input_K)\n",
    "K_flow = GlobalAveragePooling1D()(K_flow)\n",
    "x = concatenate([flat, K_flow])\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(1, activation='relu')(x)\n",
    "\n",
    "model = Model(inputs=[*base_model.inputs, input_K], outputs=x, name=net)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch 1/750000\n",
      "2/2 [==============================] - 32s 16s/step - loss: 1.6044 - val_loss: 0.4788\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47885, saving model to ./weights/weights_tail_K/DenseNet201.hdf5\n",
      "Epoch 2/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.9050 - val_loss: 0.4184\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47885 to 0.41845, saving model to ./weights/weights_tail_K/DenseNet201.hdf5\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.418.hdf5\n",
      "Epoch 3/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 1.1691 - val_loss: 0.5711\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.41845\n",
      "Epoch 4/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.7750 - val_loss: 0.6201\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.41845\n",
      "Epoch 5/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1870 - val_loss: 0.8547\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.41845\n",
      "Epoch 6/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.9885 - val_loss: 0.8632\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.41845\n",
      "Epoch 7/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.7350 - val_loss: 0.8049\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.41845\n",
      "Epoch 8/750000\n",
      "2/2 [==============================] - 1s 444ms/step - loss: 0.7239 - val_loss: 0.5866\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.41845\n",
      "Epoch 9/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 1.1583 - val_loss: 0.4995\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.41845\n",
      "Epoch 10/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9323 - val_loss: 0.5202\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.41845\n",
      "Epoch 11/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.7047 - val_loss: 0.5558\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.41845\n",
      "Epoch 12/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.9465 - val_loss: 0.4688\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.41845\n",
      "Epoch 13/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.8497 - val_loss: 0.3512\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.41845 to 0.35121, saving model to ./weights/weights_tail_K/DenseNet201.hdf5\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.351.hdf5\n",
      "Epoch 14/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.6363 - val_loss: 0.4557\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.35121\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.456.hdf5\n",
      "Epoch 15/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.5203 - val_loss: 0.3609\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.35121\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.361.hdf5\n",
      "Epoch 16/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.8642 - val_loss: 0.5008\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.35121\n",
      "Epoch 17/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9430 - val_loss: 0.7826\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.35121\n",
      "Epoch 18/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.0345 - val_loss: 0.9116\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.35121\n",
      "Epoch 19/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.1439 - val_loss: 0.9705\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.35121\n",
      "Epoch 20/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2356 - val_loss: 0.9639\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.35121\n",
      "Epoch 21/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.3871 - val_loss: 0.8808\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.35121\n",
      "Epoch 22/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8400 - val_loss: 0.8004\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.35121\n",
      "Epoch 23/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.2628 - val_loss: 0.7138\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.35121\n",
      "Epoch 24/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.9884 - val_loss: 0.6481\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.35121\n",
      "Epoch 25/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.2184 - val_loss: 0.5817\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.35121\n",
      "Epoch 26/750000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.9008 - val_loss: 0.6045\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.35121\n",
      "Epoch 27/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.2206 - val_loss: 0.7045\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.35121\n",
      "Epoch 28/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.5854 - val_loss: 0.6397\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.35121\n",
      "Epoch 29/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.8890 - val_loss: 0.5182\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.35121\n",
      "Epoch 30/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.8025 - val_loss: 0.5056\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.35121\n",
      "Epoch 31/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1752 - val_loss: 0.5009\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.35121\n",
      "Epoch 32/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9499 - val_loss: 0.4913\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.35121\n",
      "Epoch 33/750000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.7801 - val_loss: 0.5135\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.35121\n",
      "Epoch 34/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1988 - val_loss: 0.5099\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.35121\n",
      "Epoch 35/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.7861 - val_loss: 0.5281\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.35121\n",
      "Epoch 36/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8673 - val_loss: 0.5717\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.35121\n",
      "Epoch 37/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.4219 - val_loss: 0.5498\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.35121\n",
      "Epoch 38/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.8725 - val_loss: 0.3824\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.35121\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.382.hdf5\n",
      "Epoch 39/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.9923 - val_loss: 0.3419\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.35121 to 0.34192, saving model to ./weights/weights_tail_K/DenseNet201.hdf5\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.342.hdf5\n",
      "Epoch 40/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 1.5288 - val_loss: 0.4102\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.41.hdf5\n",
      "Epoch 41/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 1.1503 - val_loss: 1.0459\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.34192\n",
      "Epoch 42/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.9626 - val_loss: 1.6489\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.34192\n",
      "Epoch 43/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.1193 - val_loss: 1.9537\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.34192\n",
      "Epoch 44/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8274 - val_loss: 1.7643\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.34192\n",
      "Epoch 45/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.8287 - val_loss: 1.7849\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.34192\n",
      "Epoch 46/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.0290 - val_loss: 2.1545\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.34192\n",
      "Epoch 47/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.8064 - val_loss: 2.1341\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.34192\n",
      "Epoch 48/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.0329 - val_loss: 2.4675\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.34192\n",
      "Epoch 49/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 422ms/step - loss: 1.2792 - val_loss: 2.3763\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.34192\n",
      "Epoch 50/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1708 - val_loss: 2.2362\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.34192\n",
      "Epoch 51/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.8659 - val_loss: 1.9533\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.34192\n",
      "Epoch 52/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 1.1963 - val_loss: 2.2293\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.34192\n",
      "Epoch 53/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 2.2160 - val_loss: 2.6841\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.34192\n",
      "Epoch 54/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.5308 - val_loss: 3.2787\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.34192\n",
      "Epoch 55/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.1435 - val_loss: 3.5736\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.34192\n",
      "Epoch 56/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.0334 - val_loss: 3.5858\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.34192\n",
      "Epoch 57/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.1375 - val_loss: 3.3337\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.34192\n",
      "Epoch 58/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.8823 - val_loss: 2.8405\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.34192\n",
      "Epoch 59/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.6075 - val_loss: 2.6002\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.34192\n",
      "Epoch 60/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.8105 - val_loss: 1.9590\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.34192\n",
      "Epoch 61/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.2686 - val_loss: 1.4896\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.34192\n",
      "Epoch 62/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7897 - val_loss: 1.1457\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.34192\n",
      "Epoch 63/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.4767 - val_loss: 1.1428\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.34192\n",
      "Epoch 64/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.4156 - val_loss: 1.2803\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.34192\n",
      "Epoch 65/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.7100 - val_loss: 1.9284\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.34192\n",
      "Epoch 66/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.4248 - val_loss: 2.2853\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.34192\n",
      "Epoch 67/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.6620 - val_loss: 2.5939\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.34192\n",
      "Epoch 68/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.4000 - val_loss: 2.8080\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.34192\n",
      "Epoch 69/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.3466 - val_loss: 3.2119\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.34192\n",
      "Epoch 70/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.0838 - val_loss: 3.6671\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.34192\n",
      "Epoch 71/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.6924 - val_loss: 3.9417\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.34192\n",
      "Epoch 72/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.6614 - val_loss: 4.0737\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.34192\n",
      "Epoch 73/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.3251 - val_loss: 4.0969\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.34192\n",
      "Epoch 74/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.7832 - val_loss: 4.1056\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.34192\n",
      "Epoch 75/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.1181 - val_loss: 3.9971\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.34192\n",
      "Epoch 76/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.4483 - val_loss: 3.9968\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.34192\n",
      "Epoch 77/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.5626 - val_loss: 4.1490\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.34192\n",
      "Epoch 78/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.0523 - val_loss: 4.0962\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.34192\n",
      "Epoch 79/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.1914 - val_loss: 3.8871\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.34192\n",
      "Epoch 80/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.0482 - val_loss: 3.5825\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.34192\n",
      "Epoch 81/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.5924 - val_loss: 3.2863\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.34192\n",
      "Epoch 82/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2769 - val_loss: 3.2100\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.34192\n",
      "Epoch 83/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.7157 - val_loss: 3.0608\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.34192\n",
      "Epoch 84/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.0994 - val_loss: 3.1329\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.34192\n",
      "Epoch 85/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.6227 - val_loss: 3.1471\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.34192\n",
      "Epoch 86/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.0252 - val_loss: 3.1790\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.34192\n",
      "Epoch 87/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1512 - val_loss: 3.2200\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.34192\n",
      "Epoch 88/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.7643 - val_loss: 3.3564\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.34192\n",
      "Epoch 89/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9830 - val_loss: 3.3592\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.34192\n",
      "Epoch 90/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9752 - val_loss: 3.4510\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.34192\n",
      "Epoch 91/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.7748 - val_loss: 3.2389\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.34192\n",
      "Epoch 92/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.1136 - val_loss: 2.9531\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.34192\n",
      "Epoch 93/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.5525 - val_loss: 2.8170\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.34192\n",
      "Epoch 94/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.5341 - val_loss: 2.5019\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.34192\n",
      "Epoch 95/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.7101 - val_loss: 2.1802\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.34192\n",
      "Epoch 96/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.4983 - val_loss: 1.8784\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.34192\n",
      "Epoch 97/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2903 - val_loss: 1.7407\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.34192\n",
      "Epoch 98/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 1.1059 - val_loss: 1.4123\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.34192\n",
      "Epoch 99/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.7940 - val_loss: 1.3355\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.34192\n",
      "Epoch 100/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 1.0513 - val_loss: 1.2750\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.34192\n",
      "Epoch 101/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.8515 - val_loss: 1.4633\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.34192\n",
      "Epoch 102/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 423ms/step - loss: 1.1983 - val_loss: 1.5274\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.34192\n",
      "Epoch 103/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.7150 - val_loss: 1.5725\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.34192\n",
      "Epoch 104/750000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.8446 - val_loss: 1.4748\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.34192\n",
      "Epoch 105/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0527 - val_loss: 1.4257\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.34192\n",
      "Epoch 106/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.4664 - val_loss: 1.2207\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.34192\n",
      "Epoch 107/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1329 - val_loss: 0.9413\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.34192\n",
      "Epoch 108/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.4527 - val_loss: 0.7201\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.34192\n",
      "Epoch 109/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.6404 - val_loss: 0.5195\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.34192\n",
      "Epoch 110/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.2466 - val_loss: 0.4246\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.425.hdf5\n",
      "Epoch 111/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8102 - val_loss: 0.3500\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.35.hdf5\n",
      "Epoch 112/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.5179 - val_loss: 0.3923\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.392.hdf5\n",
      "Epoch 113/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.0881 - val_loss: 0.4052\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.405.hdf5\n",
      "Epoch 114/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.8286 - val_loss: 0.4298\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.43.hdf5\n",
      "Epoch 115/750000\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.8930 - val_loss: 0.4743\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.34192\n",
      "Epoch 116/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.5316 - val_loss: 0.5160\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.34192\n",
      "Epoch 117/750000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.8696 - val_loss: 0.4421\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.442.hdf5\n",
      "Epoch 118/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 1.6117 - val_loss: 0.4708\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.34192\n",
      "Epoch 119/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9310 - val_loss: 0.5938\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.34192\n",
      "Epoch 120/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.1219 - val_loss: 0.6747\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.34192\n",
      "Epoch 121/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.5777 - val_loss: 0.6751\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.34192\n",
      "Epoch 122/750000\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.5996 - val_loss: 0.6454\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.34192\n",
      "Epoch 123/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.6608 - val_loss: 0.6336\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.34192\n",
      "Epoch 124/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.2876 - val_loss: 0.5006\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.34192\n",
      "Epoch 125/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.8862 - val_loss: 0.3444\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.344.hdf5\n",
      "Epoch 126/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8925 - val_loss: 0.4253\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.425.hdf5\n",
      "Epoch 127/750000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.9922 - val_loss: 0.4339\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.434.hdf5\n",
      "Epoch 128/750000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 1.7500 - val_loss: 0.3794\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.379.hdf5\n",
      "Epoch 129/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.7900 - val_loss: 0.4096\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.41.hdf5\n",
      "Epoch 130/750000\n",
      "2/2 [==============================] - 1s 444ms/step - loss: 0.9754 - val_loss: 0.5559\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.34192\n",
      "Epoch 131/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.9199 - val_loss: 0.8017\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.34192\n",
      "Epoch 132/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.8318 - val_loss: 1.1588\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.34192\n",
      "Epoch 133/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.7522 - val_loss: 1.3519\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.34192\n",
      "Epoch 134/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8302 - val_loss: 1.3749\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.34192\n",
      "Epoch 135/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.3839 - val_loss: 1.1725\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.34192\n",
      "Epoch 136/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.9710 - val_loss: 0.8365\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.34192\n",
      "Epoch 137/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.2340 - val_loss: 0.5389\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.34192\n",
      "Epoch 138/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8889 - val_loss: 0.4015\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.402.hdf5\n",
      "Epoch 139/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.7783 - val_loss: 0.3974\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.397.hdf5\n",
      "Epoch 140/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4726 - val_loss: 0.4395\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.439.hdf5\n",
      "Epoch 141/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 1.0261 - val_loss: 0.4619\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.34192\n",
      "Epoch 142/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9636 - val_loss: 0.5270\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.34192\n",
      "Epoch 143/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8440 - val_loss: 0.5740\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.34192\n",
      "Epoch 144/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.5363 - val_loss: 0.7516\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.34192\n",
      "Epoch 145/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.8393 - val_loss: 0.8399\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.34192\n",
      "Epoch 146/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2043 - val_loss: 0.9465\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.34192\n",
      "Epoch 147/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.3496 - val_loss: 0.9414\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.34192\n",
      "Epoch 148/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.2410 - val_loss: 0.8657\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.34192\n",
      "Epoch 149/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9740 - val_loss: 0.7347\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.34192\n",
      "Epoch 150/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9311 - val_loss: 0.7069\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.34192\n",
      "Epoch 151/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1178 - val_loss: 0.6794\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.34192\n",
      "Epoch 152/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.7744 - val_loss: 0.5677\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.34192\n",
      "Epoch 153/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.6604 - val_loss: 0.5044\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.34192\n",
      "Epoch 154/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.1977 - val_loss: 0.5343\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.34192\n",
      "Epoch 155/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.8566 - val_loss: 0.5622\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.34192\n",
      "Epoch 156/750000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.5875 - val_loss: 0.6163\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.34192\n",
      "Epoch 157/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.4527 - val_loss: 0.5446\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.34192\n",
      "Epoch 158/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 1.0582 - val_loss: 0.6260\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.34192\n",
      "Epoch 159/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.1177 - val_loss: 0.7813\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.34192\n",
      "Epoch 160/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8651 - val_loss: 0.8972\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.34192\n",
      "Epoch 161/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.2278 - val_loss: 0.9079\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.34192\n",
      "Epoch 162/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9435 - val_loss: 0.7587\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.34192\n",
      "Epoch 163/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8061 - val_loss: 0.6633\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.34192\n",
      "Epoch 164/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9553 - val_loss: 0.6450\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.34192\n",
      "Epoch 165/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1208 - val_loss: 0.6109\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.34192\n",
      "Epoch 166/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.2893 - val_loss: 0.6346\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.34192\n",
      "Epoch 167/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.6213 - val_loss: 0.6135\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.34192\n",
      "Epoch 168/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.8953 - val_loss: 0.6195\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.34192\n",
      "Epoch 169/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 1.3957 - val_loss: 0.6149\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.34192\n",
      "Epoch 170/750000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 1.3007 - val_loss: 0.7090\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.34192\n",
      "Epoch 171/750000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 1.0279 - val_loss: 0.6224\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.34192\n",
      "Epoch 172/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 1.1352 - val_loss: 0.5982\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.34192\n",
      "Epoch 173/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.7427 - val_loss: 0.5170\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.34192\n",
      "Epoch 174/750000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.8900 - val_loss: 0.5302\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.34192\n",
      "Epoch 175/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.7784 - val_loss: 0.4719\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.34192\n",
      "Epoch 176/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.5850 - val_loss: 0.5315\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.34192\n",
      "Epoch 177/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1098 - val_loss: 0.5554\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.34192\n",
      "Epoch 178/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.8160 - val_loss: 0.7290\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.34192\n",
      "Epoch 179/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.5155 - val_loss: 0.9882\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.34192\n",
      "Epoch 180/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1874 - val_loss: 1.4137\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.34192\n",
      "Epoch 181/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.5608 - val_loss: 1.5600\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.34192\n",
      "Epoch 182/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.8474 - val_loss: 1.6273\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.34192\n",
      "Epoch 183/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9897 - val_loss: 1.5539\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.34192\n",
      "Epoch 184/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9146 - val_loss: 1.4632\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.34192\n",
      "Epoch 185/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0321 - val_loss: 1.5553\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.34192\n",
      "Epoch 186/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 1.2868 - val_loss: 1.5124\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.34192\n",
      "Epoch 187/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.4950 - val_loss: 1.3095\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.34192\n",
      "Epoch 188/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.5341 - val_loss: 1.0994\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.34192\n",
      "Epoch 189/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.0962 - val_loss: 0.9512\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.34192\n",
      "Epoch 190/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.2528 - val_loss: 0.9889\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.34192\n",
      "Epoch 191/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.4648 - val_loss: 1.0256\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.34192\n",
      "Epoch 192/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9010 - val_loss: 0.9264\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.34192\n",
      "Epoch 193/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9731 - val_loss: 0.9322\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.34192\n",
      "Epoch 194/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.7208 - val_loss: 0.9739\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.34192\n",
      "Epoch 195/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.8341 - val_loss: 1.0615\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.34192\n",
      "Epoch 196/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.5698 - val_loss: 1.0065\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.34192\n",
      "Epoch 197/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.6747 - val_loss: 0.9111\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.34192\n",
      "Epoch 198/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.6730 - val_loss: 0.8621\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.34192\n",
      "Epoch 199/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.9650 - val_loss: 0.8999\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.34192\n",
      "Epoch 200/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8085 - val_loss: 0.9208\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.34192\n",
      "Epoch 201/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.6907 - val_loss: 0.9747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00201: val_loss did not improve from 0.34192\n",
      "Epoch 202/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.3985 - val_loss: 1.0717\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.34192\n",
      "Epoch 203/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9786 - val_loss: 1.3563\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.34192\n",
      "Epoch 204/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.5422 - val_loss: 1.6057\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.34192\n",
      "Epoch 205/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1481 - val_loss: 1.7698\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.34192\n",
      "Epoch 206/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.9670 - val_loss: 1.6351\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.34192\n",
      "Epoch 207/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8739 - val_loss: 1.4714\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.34192\n",
      "Epoch 208/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.6070 - val_loss: 1.1851\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.34192\n",
      "Epoch 209/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0577 - val_loss: 1.0839\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.34192\n",
      "Epoch 210/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.9776 - val_loss: 0.9531\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.34192\n",
      "Epoch 211/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.8687 - val_loss: 1.0392\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.34192\n",
      "Epoch 212/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.0331 - val_loss: 1.1552\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.34192\n",
      "Epoch 213/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9537 - val_loss: 1.4582\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.34192\n",
      "Epoch 214/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.6326 - val_loss: 1.7467\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.34192\n",
      "Epoch 215/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4793 - val_loss: 2.0085\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.34192\n",
      "Epoch 216/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.2132 - val_loss: 1.9543\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.34192\n",
      "Epoch 217/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.8112 - val_loss: 1.8701\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.34192\n",
      "Epoch 218/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.5310 - val_loss: 1.6063\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.34192\n",
      "Epoch 219/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.3534 - val_loss: 1.3149\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.34192\n",
      "Epoch 220/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9317 - val_loss: 0.9875\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.34192\n",
      "Epoch 221/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.8641 - val_loss: 0.8108\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.34192\n",
      "Epoch 222/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.7588 - val_loss: 0.6872\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.34192\n",
      "Epoch 223/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.0931 - val_loss: 0.5075\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.34192\n",
      "Epoch 224/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.6014 - val_loss: 0.5363\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.34192\n",
      "Epoch 225/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.4848 - val_loss: 0.5187\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.34192\n",
      "Epoch 226/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.7415 - val_loss: 0.6378\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.34192\n",
      "Epoch 227/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.1629 - val_loss: 0.6219\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.34192\n",
      "Epoch 228/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.7992 - val_loss: 0.6387\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.34192\n",
      "Epoch 229/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9743 - val_loss: 0.5614\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.34192\n",
      "Epoch 230/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.9144 - val_loss: 0.5295\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.34192\n",
      "Epoch 231/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 1.0732 - val_loss: 0.4767\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.34192\n",
      "Epoch 232/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.0289 - val_loss: 0.4943\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.34192\n",
      "Epoch 233/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.8751 - val_loss: 0.4670\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.34192\n",
      "Epoch 234/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.0811 - val_loss: 0.5202\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.34192\n",
      "Epoch 235/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 1.0554 - val_loss: 0.5048\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.34192\n",
      "Epoch 236/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.7523 - val_loss: 0.5696\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.34192\n",
      "Epoch 237/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.4633 - val_loss: 0.5632\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.34192\n",
      "Epoch 238/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.5113 - val_loss: 0.6395\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.34192\n",
      "Epoch 239/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.9257 - val_loss: 0.6244\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.34192\n",
      "Epoch 240/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8604 - val_loss: 0.6778\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.34192\n",
      "Epoch 241/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.7673 - val_loss: 0.6404\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.34192\n",
      "Epoch 242/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.9777 - val_loss: 0.6281\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.34192\n",
      "Epoch 243/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9514 - val_loss: 0.5230\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.34192\n",
      "Epoch 244/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.7336 - val_loss: 0.5258\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.34192\n",
      "Epoch 245/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9751 - val_loss: 0.4920\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.34192\n",
      "Epoch 246/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.0801 - val_loss: 0.5317\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.34192\n",
      "Epoch 247/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0572 - val_loss: 0.4971\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.34192\n",
      "Epoch 248/750000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.6869 - val_loss: 0.5329\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.34192\n",
      "Epoch 249/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9872 - val_loss: 0.5239\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.34192\n",
      "Epoch 250/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.1627 - val_loss: 0.5309\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.34192\n",
      "Epoch 251/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8246 - val_loss: 0.5136\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.34192\n",
      "Epoch 252/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.3067 - val_loss: 0.5253\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.34192\n",
      "Epoch 253/750000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 1.1575 - val_loss: 0.4860\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.34192\n",
      "Epoch 254/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 421ms/step - loss: 1.1872 - val_loss: 0.4426\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.443.hdf5\n",
      "Epoch 255/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 1.2885 - val_loss: 0.3820\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.382.hdf5\n",
      "Epoch 256/750000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.8919 - val_loss: 0.3746\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.375.hdf5\n",
      "Epoch 257/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0334 - val_loss: 0.3669\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.367.hdf5\n",
      "Epoch 258/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.3298 - val_loss: 0.4271\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.427.hdf5\n",
      "Epoch 259/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.8710 - val_loss: 0.4591\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.34192\n",
      "Epoch 260/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.6764 - val_loss: 0.4461\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.446.hdf5\n",
      "Epoch 261/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.4858 - val_loss: 0.4356\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.436.hdf5\n",
      "Epoch 262/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.7336 - val_loss: 0.4159\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.416.hdf5\n",
      "Epoch 263/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.3034 - val_loss: 0.4417\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.442.hdf5\n",
      "Epoch 264/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.8284 - val_loss: 0.5235\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.34192\n",
      "Epoch 265/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.2622 - val_loss: 0.6032\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.34192\n",
      "Epoch 266/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.9510 - val_loss: 0.7236\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.34192\n",
      "Epoch 267/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.0050 - val_loss: 0.8907\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.34192\n",
      "Epoch 268/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9633 - val_loss: 1.0393\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.34192\n",
      "Epoch 269/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.6342 - val_loss: 1.1993\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.34192\n",
      "Epoch 270/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.8757 - val_loss: 1.3492\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.34192\n",
      "Epoch 271/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0229 - val_loss: 1.4794\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.34192\n",
      "Epoch 272/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.7817 - val_loss: 1.4672\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.34192\n",
      "Epoch 273/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.5918 - val_loss: 1.2635\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.34192\n",
      "Epoch 274/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.3754 - val_loss: 1.0445\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.34192\n",
      "Epoch 275/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.2524 - val_loss: 0.7685\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.34192\n",
      "Epoch 276/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.3748 - val_loss: 0.6020\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.34192\n",
      "Epoch 277/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.9830 - val_loss: 0.4707\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.34192\n",
      "Epoch 278/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8155 - val_loss: 0.4419\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.442.hdf5\n",
      "Epoch 279/750000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.8782 - val_loss: 0.4067\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.407.hdf5\n",
      "Epoch 280/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 1.0581 - val_loss: 0.5250\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.34192\n",
      "Epoch 281/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.4010 - val_loss: 0.7322\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.34192\n",
      "Epoch 282/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.9978 - val_loss: 0.8930\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.34192\n",
      "Epoch 283/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.0537 - val_loss: 1.0488\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.34192\n",
      "Epoch 284/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.6919 - val_loss: 1.2554\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.34192\n",
      "Epoch 285/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 1.3508 - val_loss: 1.2635\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.34192\n",
      "Epoch 286/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.0686 - val_loss: 1.3441\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.34192\n",
      "Epoch 287/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.1437 - val_loss: 1.4430\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.34192\n",
      "Epoch 288/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.6490 - val_loss: 1.5010\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.34192\n",
      "Epoch 289/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.7952 - val_loss: 1.4392\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.34192\n",
      "Epoch 290/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.3646 - val_loss: 1.4819\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.34192\n",
      "Epoch 291/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.9358 - val_loss: 1.4407\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.34192\n",
      "Epoch 292/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8543 - val_loss: 1.5759\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.34192\n",
      "Epoch 293/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.6930 - val_loss: 1.5743\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.34192\n",
      "Epoch 294/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9147 - val_loss: 1.6497\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.34192\n",
      "Epoch 295/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.0914 - val_loss: 1.6211\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.34192\n",
      "Epoch 296/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2088 - val_loss: 1.8073\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.34192\n",
      "Epoch 297/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0560 - val_loss: 1.7933\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.34192\n",
      "Epoch 298/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.5601 - val_loss: 1.9698\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.34192\n",
      "Epoch 299/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.1601 - val_loss: 1.9698\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.34192\n",
      "Epoch 300/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8311 - val_loss: 1.8916\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.34192\n",
      "Epoch 301/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9113 - val_loss: 1.8500\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.34192\n",
      "Epoch 302/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 415ms/step - loss: 1.2537 - val_loss: 2.0112\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.34192\n",
      "Epoch 303/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.7476 - val_loss: 2.1407\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.34192\n",
      "Epoch 304/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.9751 - val_loss: 2.1862\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.34192\n",
      "Epoch 305/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1263 - val_loss: 2.1608\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.34192\n",
      "Epoch 306/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.1255 - val_loss: 2.0894\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.34192\n",
      "Epoch 307/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.7220 - val_loss: 1.9143\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.34192\n",
      "Epoch 308/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8446 - val_loss: 1.7453\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.34192\n",
      "Epoch 309/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.3231 - val_loss: 1.6579\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.34192\n",
      "Epoch 310/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.4752 - val_loss: 1.7546\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.34192\n",
      "Epoch 311/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7312 - val_loss: 1.8295\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.34192\n",
      "Epoch 312/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.3746 - val_loss: 1.8412\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.34192\n",
      "Epoch 313/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1725 - val_loss: 1.7483\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.34192\n",
      "Epoch 314/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.9694 - val_loss: 1.5170\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.34192\n",
      "Epoch 315/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.7584 - val_loss: 1.3349\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.34192\n",
      "Epoch 316/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8600 - val_loss: 1.2591\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.34192\n",
      "Epoch 317/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 1.6741 - val_loss: 1.4122\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.34192\n",
      "Epoch 318/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1169 - val_loss: 1.6165\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.34192\n",
      "Epoch 319/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.2331 - val_loss: 1.6838\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.34192\n",
      "Epoch 320/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.6957 - val_loss: 1.7695\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.34192\n",
      "Epoch 321/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7274 - val_loss: 1.8924\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.34192\n",
      "Epoch 322/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.0808 - val_loss: 1.9239\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.34192\n",
      "Epoch 323/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.3672 - val_loss: 1.8901\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.34192\n",
      "Epoch 324/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.8900 - val_loss: 1.6302\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.34192\n",
      "Epoch 325/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 1.1262 - val_loss: 1.3717\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.34192\n",
      "Epoch 326/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.1561 - val_loss: 1.1122\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.34192\n",
      "Epoch 327/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8495 - val_loss: 0.8732\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.34192\n",
      "Epoch 328/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.7856 - val_loss: 0.6022\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.34192\n",
      "Epoch 329/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.5503 - val_loss: 0.4954\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.34192\n",
      "Epoch 330/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.6593 - val_loss: 0.4568\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.34192\n",
      "Epoch 331/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.3656 - val_loss: 0.4592\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.34192\n",
      "Epoch 332/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.8957 - val_loss: 0.4427\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.443.hdf5\n",
      "Epoch 333/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 1.4282 - val_loss: 0.4767\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.34192\n",
      "Epoch 334/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.0381 - val_loss: 0.4756\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.34192\n",
      "Epoch 335/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.7534 - val_loss: 0.4580\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.34192\n",
      "Epoch 336/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.4101 - val_loss: 0.4647\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.34192\n",
      "Epoch 337/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.5244 - val_loss: 0.4615\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.34192\n",
      "Epoch 338/750000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 1.0010 - val_loss: 0.4876\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.34192\n",
      "Epoch 339/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1724 - val_loss: 0.5935\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.34192\n",
      "Epoch 340/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.2875 - val_loss: 0.6565\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.34192\n",
      "Epoch 341/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8575 - val_loss: 0.5816\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.34192\n",
      "Epoch 342/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9273 - val_loss: 0.5888\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.34192\n",
      "Epoch 343/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.8025 - val_loss: 0.5673\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.34192\n",
      "Epoch 344/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8524 - val_loss: 0.6398\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.34192\n",
      "Epoch 345/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.9324 - val_loss: 0.5975\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.34192\n",
      "Epoch 346/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.2539 - val_loss: 0.6334\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.34192\n",
      "Epoch 347/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.1949 - val_loss: 0.6223\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.34192\n",
      "Epoch 348/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.6951 - val_loss: 0.7226\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.34192\n",
      "Epoch 349/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.8961 - val_loss: 0.6801\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.34192\n",
      "Epoch 350/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.9115 - val_loss: 0.7238\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.34192\n",
      "Epoch 351/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.7212 - val_loss: 0.6904\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.34192\n",
      "Epoch 352/750000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 1.0727 - val_loss: 0.7330\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.34192\n",
      "Epoch 353/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.5245 - val_loss: 0.6899\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.34192\n",
      "Epoch 354/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 417ms/step - loss: 1.3366 - val_loss: 0.6985\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.34192\n",
      "Epoch 355/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.7143 - val_loss: 0.6802\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.34192\n",
      "Epoch 356/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.5740 - val_loss: 0.6892\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.34192\n",
      "Epoch 357/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.1672 - val_loss: 0.6742\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.34192\n",
      "Epoch 358/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.7326 - val_loss: 0.6812\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.34192\n",
      "Epoch 359/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.2866 - val_loss: 0.6545\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.34192\n",
      "Epoch 360/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.9093 - val_loss: 0.6349\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.34192\n",
      "Epoch 361/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.9325 - val_loss: 0.5968\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.34192\n",
      "Epoch 362/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.5335 - val_loss: 0.6035\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.34192\n",
      "Epoch 363/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0436 - val_loss: 0.5906\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.34192\n",
      "Epoch 364/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.8502 - val_loss: 0.5921\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.34192\n",
      "Epoch 365/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7456 - val_loss: 0.5503\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.34192\n",
      "Epoch 366/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.4547 - val_loss: 0.5183\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.34192\n",
      "Epoch 367/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.9993 - val_loss: 0.4523\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.452.hdf5\n",
      "Epoch 368/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.9457 - val_loss: 0.4536\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.454.hdf5\n",
      "Epoch 369/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.7268 - val_loss: 0.4224\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.422.hdf5\n",
      "Epoch 370/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 1.0177 - val_loss: 0.4572\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.34192\n",
      "Epoch 371/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.6430 - val_loss: 0.5470\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.34192\n",
      "Epoch 372/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.2832 - val_loss: 0.6546\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.34192\n",
      "Epoch 373/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.6503 - val_loss: 0.6529\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.34192\n",
      "Epoch 374/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.4300 - val_loss: 0.6109\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.34192\n",
      "Epoch 375/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.9045 - val_loss: 0.4899\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.34192\n",
      "Epoch 376/750000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 1.2736 - val_loss: 0.4615\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.34192\n",
      "Epoch 377/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.4127 - val_loss: 0.4737\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.34192\n",
      "Epoch 378/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.0319 - val_loss: 0.6414\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.34192\n",
      "Epoch 379/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.0747 - val_loss: 0.7058\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.34192\n",
      "Epoch 380/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.9838 - val_loss: 0.7904\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.34192\n",
      "Epoch 381/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.6656 - val_loss: 0.7553\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.34192\n",
      "Epoch 382/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9001 - val_loss: 0.6541\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.34192\n",
      "Epoch 383/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.3895 - val_loss: 0.5213\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.34192\n",
      "Epoch 384/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.7948 - val_loss: 0.4731\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.34192\n",
      "Epoch 385/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.4485 - val_loss: 0.4211\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.421.hdf5\n",
      "Epoch 386/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.8603 - val_loss: 0.5244\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.34192\n",
      "Epoch 387/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0942 - val_loss: 0.6360\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.34192\n",
      "Epoch 388/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.7421 - val_loss: 0.8099\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.34192\n",
      "Epoch 389/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.8560 - val_loss: 0.9865\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.34192\n",
      "Epoch 390/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.7897 - val_loss: 1.1750\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.34192\n",
      "Epoch 391/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.0156 - val_loss: 1.4191\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.34192\n",
      "Epoch 392/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 1.0803 - val_loss: 1.6541\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.34192\n",
      "Epoch 393/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.7183 - val_loss: 1.6899\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.34192\n",
      "Epoch 394/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.5577 - val_loss: 1.6226\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.34192\n",
      "Epoch 395/750000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 1.0594 - val_loss: 1.4850\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.34192\n",
      "Epoch 396/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.1846 - val_loss: 1.2791\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.34192\n",
      "Epoch 397/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.5112 - val_loss: 1.0556\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.34192\n",
      "Epoch 398/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.6238 - val_loss: 0.9929\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.34192\n",
      "Epoch 399/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.7251 - val_loss: 0.9718\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.34192\n",
      "Epoch 400/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1886 - val_loss: 0.9928\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.34192\n",
      "Epoch 401/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.1856 - val_loss: 0.9072\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.34192\n",
      "Epoch 402/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9550 - val_loss: 0.8749\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.34192\n",
      "Epoch 403/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.2079 - val_loss: 0.7404\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.34192\n",
      "Epoch 404/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.1931 - val_loss: 0.7753\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.34192\n",
      "Epoch 405/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 414ms/step - loss: 0.9960 - val_loss: 0.8580\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.34192\n",
      "Epoch 406/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.7567 - val_loss: 1.0714\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.34192\n",
      "Epoch 407/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.0211 - val_loss: 1.2942\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.34192\n",
      "Epoch 408/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 1.1754 - val_loss: 1.5373\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.34192\n",
      "Epoch 409/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.5825 - val_loss: 1.7266\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.34192\n",
      "Epoch 410/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.9908 - val_loss: 1.7471\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.34192\n",
      "Epoch 411/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.9253 - val_loss: 1.8090\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.34192\n",
      "Epoch 412/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.9068 - val_loss: 1.9570\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.34192\n",
      "Epoch 413/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.5367 - val_loss: 2.0137\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.34192\n",
      "Epoch 414/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.9093 - val_loss: 1.9759\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.34192\n",
      "Epoch 415/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.7684 - val_loss: 1.8058\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.34192\n",
      "Epoch 416/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.7773 - val_loss: 1.6194\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.34192\n",
      "Epoch 417/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9400 - val_loss: 1.5309\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.34192\n",
      "Epoch 418/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.5032 - val_loss: 1.7140\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.34192\n",
      "Epoch 419/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8719 - val_loss: 1.9625\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.34192\n",
      "Epoch 420/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.5802 - val_loss: 2.1252\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.34192\n",
      "Epoch 421/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.2906 - val_loss: 2.1655\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.34192\n",
      "Epoch 422/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7959 - val_loss: 2.1340\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.34192\n",
      "Epoch 423/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.5543 - val_loss: 2.1773\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.34192\n",
      "Epoch 424/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.1609 - val_loss: 2.0988\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.34192\n",
      "Epoch 425/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9073 - val_loss: 2.0438\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.34192\n",
      "Epoch 426/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.3590 - val_loss: 2.0726\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.34192\n",
      "Epoch 427/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.0639 - val_loss: 2.0524\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.34192\n",
      "Epoch 428/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.8596 - val_loss: 2.0454\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.34192\n",
      "Epoch 429/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.1721 - val_loss: 2.1382\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.34192\n",
      "Epoch 430/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.1136 - val_loss: 2.3249\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.34192\n",
      "Epoch 431/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.1737 - val_loss: 2.3696\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.34192\n",
      "Epoch 432/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9715 - val_loss: 2.3679\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.34192\n",
      "Epoch 433/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.4026 - val_loss: 2.3464\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.34192\n",
      "Epoch 434/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.6068 - val_loss: 2.3319\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.34192\n",
      "Epoch 435/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 1.1669 - val_loss: 2.2168\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.34192\n",
      "Epoch 436/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.9262 - val_loss: 1.9920\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.34192\n",
      "Epoch 437/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.6701 - val_loss: 1.8497\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.34192\n",
      "Epoch 438/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.9507 - val_loss: 1.6683\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.34192\n",
      "Epoch 439/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.1073 - val_loss: 1.4241\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.34192\n",
      "Epoch 440/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.5259 - val_loss: 1.1579\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.34192\n",
      "Epoch 441/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.5294 - val_loss: 1.0347\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.34192\n",
      "Epoch 442/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.5394 - val_loss: 1.0295\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.34192\n",
      "Epoch 443/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.0022 - val_loss: 1.0147\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.34192\n",
      "Epoch 444/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.6951 - val_loss: 0.9414\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.34192\n",
      "Epoch 445/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.3152 - val_loss: 0.9636\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.34192\n",
      "Epoch 446/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.7681 - val_loss: 0.8679\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.34192\n",
      "Epoch 447/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.9686 - val_loss: 0.9022\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.34192\n",
      "Epoch 448/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.9537 - val_loss: 0.8763\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.34192\n",
      "Epoch 449/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8311 - val_loss: 0.9649\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.34192\n",
      "Epoch 450/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1463 - val_loss: 0.7774\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.34192\n",
      "Epoch 451/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.0296 - val_loss: 0.7211\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.34192\n",
      "Epoch 452/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.0032 - val_loss: 0.5527\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.34192\n",
      "Epoch 453/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 1.0919 - val_loss: 0.5000\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.34192\n",
      "Epoch 454/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.5162 - val_loss: 0.4937\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.34192\n",
      "Epoch 455/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9043 - val_loss: 0.6265\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.34192\n",
      "Epoch 456/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1086 - val_loss: 0.8332\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.34192\n",
      "Epoch 457/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.8910 - val_loss: 1.1681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00457: val_loss did not improve from 0.34192\n",
      "Epoch 458/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.9252 - val_loss: 1.2989\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.34192\n",
      "Epoch 459/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.4388 - val_loss: 1.4837\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.34192\n",
      "Epoch 460/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.6533 - val_loss: 1.4242\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.34192\n",
      "Epoch 461/750000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.9560 - val_loss: 1.3466\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.34192\n",
      "Epoch 462/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7203 - val_loss: 1.1578\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.34192\n",
      "Epoch 463/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.7144 - val_loss: 1.1518\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.34192\n",
      "Epoch 464/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8920 - val_loss: 1.0869\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.34192\n",
      "Epoch 465/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8566 - val_loss: 1.0614\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.34192\n",
      "Epoch 466/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8308 - val_loss: 1.0888\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.34192\n",
      "Epoch 467/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.4284 - val_loss: 1.2335\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.34192\n",
      "Epoch 468/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.1493 - val_loss: 1.3535\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.34192\n",
      "Epoch 469/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9375 - val_loss: 1.4352\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.34192\n",
      "Epoch 470/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9995 - val_loss: 1.3990\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.34192\n",
      "Epoch 471/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9601 - val_loss: 1.2675\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.34192\n",
      "Epoch 472/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1925 - val_loss: 1.0779\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.34192\n",
      "Epoch 473/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8643 - val_loss: 0.8790\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.34192\n",
      "Epoch 474/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.0377 - val_loss: 0.6735\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.34192\n",
      "Epoch 475/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.8367 - val_loss: 0.5289\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.34192\n",
      "Epoch 476/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.2399 - val_loss: 0.4213\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.421.hdf5\n",
      "Epoch 477/750000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.5507 - val_loss: 0.4322\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.432.hdf5\n",
      "Epoch 478/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 1.4111 - val_loss: 0.5301\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.34192\n",
      "Epoch 479/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.6490 - val_loss: 0.5956\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.34192\n",
      "Epoch 480/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.9680 - val_loss: 0.7030\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.34192\n",
      "Epoch 481/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.3966 - val_loss: 0.8930\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.34192\n",
      "Epoch 482/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.0561 - val_loss: 1.0744\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.34192\n",
      "Epoch 483/750000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 1.2050 - val_loss: 1.1598\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.34192\n",
      "Epoch 484/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.2389 - val_loss: 1.1308\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.34192\n",
      "Epoch 485/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.4017 - val_loss: 1.0780\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.34192\n",
      "Epoch 486/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0944 - val_loss: 1.0227\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.34192\n",
      "Epoch 487/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.8658 - val_loss: 0.9882\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.34192\n",
      "Epoch 488/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8509 - val_loss: 0.9657\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.34192\n",
      "Epoch 489/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1211 - val_loss: 0.9676\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.34192\n",
      "Epoch 490/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8434 - val_loss: 0.9779\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.34192\n",
      "Epoch 491/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9865 - val_loss: 0.9976\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.34192\n",
      "Epoch 492/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.8900 - val_loss: 1.0029\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.34192\n",
      "Epoch 493/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1887 - val_loss: 1.0075\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.34192\n",
      "Epoch 494/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.9734 - val_loss: 1.0763\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.34192\n",
      "Epoch 495/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.0156 - val_loss: 1.0475\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.34192\n",
      "Epoch 496/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.8481 - val_loss: 0.9222\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.34192\n",
      "Epoch 497/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.9532 - val_loss: 0.8683\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.34192\n",
      "Epoch 498/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9951 - val_loss: 0.8486\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.34192\n",
      "Epoch 499/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.7409 - val_loss: 0.8056\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.34192\n",
      "Epoch 500/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.8555 - val_loss: 0.7346\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.34192\n",
      "Epoch 501/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.7745 - val_loss: 0.8284\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.34192\n",
      "Epoch 502/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.8753 - val_loss: 0.9397\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.34192\n",
      "Epoch 503/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.5991 - val_loss: 0.9991\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.34192\n",
      "Epoch 504/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.2435 - val_loss: 0.8822\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.34192\n",
      "Epoch 505/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.5077 - val_loss: 0.8027\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.34192\n",
      "Epoch 506/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 1.0291 - val_loss: 0.6870\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.34192\n",
      "Epoch 507/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.1868 - val_loss: 0.6713\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.34192\n",
      "Epoch 508/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.6399 - val_loss: 0.4989\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.34192\n",
      "Epoch 509/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 426ms/step - loss: 1.3056 - val_loss: 0.4190\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.34192\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.419.hdf5\n",
      "Epoch 510/750000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.9993 - val_loss: 0.3290\n",
      "\n",
      "Epoch 00510: val_loss improved from 0.34192 to 0.32902, saving model to ./weights/weights_tail_K/DenseNet201.hdf5\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.329.hdf5\n",
      "Epoch 511/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.8698 - val_loss: 0.3501\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.35.hdf5\n",
      "Epoch 512/750000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 1.2666 - val_loss: 0.3549\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.355.hdf5\n",
      "Epoch 513/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 1.2621 - val_loss: 0.3745\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.374.hdf5\n",
      "Epoch 514/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.7547 - val_loss: 0.3386\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.339.hdf5\n",
      "Epoch 515/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 1.3333 - val_loss: 0.3350\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.335.hdf5\n",
      "Epoch 516/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.0641 - val_loss: 0.3379\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.338.hdf5\n",
      "Epoch 517/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.8743 - val_loss: 0.3835\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.384.hdf5\n",
      "Epoch 518/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.7100 - val_loss: 0.5132\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.32902\n",
      "Epoch 519/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.7603 - val_loss: 0.7369\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.32902\n",
      "Epoch 520/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1339 - val_loss: 0.8815\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.32902\n",
      "Epoch 521/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.0258 - val_loss: 0.7922\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.32902\n",
      "Epoch 522/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.2163 - val_loss: 0.6794\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.32902\n",
      "Epoch 523/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.3495 - val_loss: 0.4982\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.32902\n",
      "Epoch 524/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.4930 - val_loss: 0.4910\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.32902\n",
      "Epoch 525/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.6837 - val_loss: 0.4288\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.429.hdf5\n",
      "Epoch 526/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.7101 - val_loss: 0.4565\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.456.hdf5\n",
      "Epoch 527/750000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 1.3324 - val_loss: 0.4482\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.448.hdf5\n",
      "Epoch 528/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.1503 - val_loss: 0.5560\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.32902\n",
      "Epoch 529/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.8253 - val_loss: 0.6655\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.32902\n",
      "Epoch 530/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.1008 - val_loss: 0.8073\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.32902\n",
      "Epoch 531/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 2.0134 - val_loss: 0.9193\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.32902\n",
      "Epoch 532/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9934 - val_loss: 0.9054\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.32902\n",
      "Epoch 533/750000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 1.1226 - val_loss: 0.7637\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.32902\n",
      "Epoch 534/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9345 - val_loss: 0.6685\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.32902\n",
      "Epoch 535/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.3577 - val_loss: 0.4569\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.32902\n",
      "Epoch 536/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9916 - val_loss: 0.4122\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.412.hdf5\n",
      "Epoch 537/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.7546 - val_loss: 0.7205\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.32902\n",
      "Epoch 538/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.0418 - val_loss: 1.1816\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.32902\n",
      "Epoch 539/750000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 1.1737 - val_loss: 1.5626\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.32902\n",
      "Epoch 540/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.2113 - val_loss: 1.7015\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.32902\n",
      "Epoch 541/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.6947 - val_loss: 1.7851\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.32902\n",
      "Epoch 542/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 2.0058 - val_loss: 1.6677\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.32902\n",
      "Epoch 543/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.9426 - val_loss: 1.5104\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.32902\n",
      "Epoch 544/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.8579 - val_loss: 1.3324\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.32902\n",
      "Epoch 545/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8599 - val_loss: 1.2624\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.32902\n",
      "Epoch 546/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.6572 - val_loss: 1.0784\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.32902\n",
      "Epoch 547/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.7517 - val_loss: 0.9828\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.32902\n",
      "Epoch 548/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.6749 - val_loss: 0.8015\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.32902\n",
      "Epoch 549/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.2994 - val_loss: 0.6362\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.32902\n",
      "Epoch 550/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.1400 - val_loss: 0.5727\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.32902\n",
      "Epoch 551/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.3304 - val_loss: 0.5751\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.32902\n",
      "Epoch 552/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.8904 - val_loss: 0.5861\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.32902\n",
      "Epoch 553/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9184 - val_loss: 0.5842\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.32902\n",
      "Epoch 554/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.7752 - val_loss: 0.5769\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.32902\n",
      "Epoch 555/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.7203 - val_loss: 0.6347\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.32902\n",
      "Epoch 556/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 413ms/step - loss: 1.1662 - val_loss: 0.7225\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.32902\n",
      "Epoch 557/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.9885 - val_loss: 0.6851\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.32902\n",
      "Epoch 558/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.8830 - val_loss: 0.5901\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.32902\n",
      "Epoch 559/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.6877 - val_loss: 0.5080\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.32902\n",
      "Epoch 560/750000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.8593 - val_loss: 0.5236\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.32902\n",
      "Epoch 561/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.1243 - val_loss: 0.4630\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.32902\n",
      "Epoch 562/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.9613 - val_loss: 0.4902\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.32902\n",
      "Epoch 563/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.9891 - val_loss: 0.4888\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.32902\n",
      "Epoch 564/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2656 - val_loss: 0.5008\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.32902\n",
      "Epoch 565/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.4696 - val_loss: 0.4727\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.32902\n",
      "Epoch 566/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.6904 - val_loss: 0.4741\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.32902\n",
      "Epoch 567/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.7121 - val_loss: 0.4065\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.406.hdf5\n",
      "Epoch 568/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.6016 - val_loss: 0.4252\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.425.hdf5\n",
      "Epoch 569/750000\n",
      "2/2 [==============================] - 1s 474ms/step - loss: 1.1632 - val_loss: 0.4009\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.401.hdf5\n",
      "Epoch 570/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.5369 - val_loss: 0.4336\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.434.hdf5\n",
      "Epoch 571/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.9896 - val_loss: 0.3871\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.387.hdf5\n",
      "Epoch 572/750000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 1.7103 - val_loss: 0.4057\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.406.hdf5\n",
      "Epoch 573/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 1.3447 - val_loss: 0.3516\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.352.hdf5\n",
      "Epoch 574/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.8599 - val_loss: 0.3886\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.389.hdf5\n",
      "Epoch 575/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.6736 - val_loss: 0.3547\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.355.hdf5\n",
      "Epoch 576/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 1.6569 - val_loss: 0.4415\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.441.hdf5\n",
      "Epoch 577/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.8407 - val_loss: 0.5176\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.32902\n",
      "Epoch 578/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 1.3476 - val_loss: 0.6221\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.32902\n",
      "Epoch 579/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9501 - val_loss: 0.6159\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.32902\n",
      "Epoch 580/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.0393 - val_loss: 0.5913\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.32902\n",
      "Epoch 581/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.6736 - val_loss: 0.4585\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.32902\n",
      "Epoch 582/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.6784 - val_loss: 0.4825\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.32902\n",
      "Epoch 583/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.9680 - val_loss: 0.4207\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.421.hdf5\n",
      "Epoch 584/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.8507 - val_loss: 0.4420\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.442.hdf5\n",
      "Epoch 585/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.5570 - val_loss: 0.4241\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.424.hdf5\n",
      "Epoch 586/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8312 - val_loss: 0.5312\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.32902\n",
      "Epoch 587/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.3762 - val_loss: 0.6345\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.32902\n",
      "Epoch 588/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.8750 - val_loss: 0.8486\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.32902\n",
      "Epoch 589/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 1.7880 - val_loss: 0.9037\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.32902\n",
      "Epoch 590/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.0223 - val_loss: 0.8733\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.32902\n",
      "Epoch 591/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.8455 - val_loss: 0.8300\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.32902\n",
      "Epoch 592/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9710 - val_loss: 0.8515\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.32902\n",
      "Epoch 593/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.9092 - val_loss: 0.8676\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.32902\n",
      "Epoch 594/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.3876 - val_loss: 0.8069\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.32902\n",
      "Epoch 595/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0536 - val_loss: 0.8454\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.32902\n",
      "Epoch 596/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.3194 - val_loss: 0.9778\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.32902\n",
      "Epoch 597/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.3240 - val_loss: 1.0291\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.32902\n",
      "Epoch 598/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.7565 - val_loss: 1.0271\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.32902\n",
      "Epoch 599/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.9567 - val_loss: 0.9918\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.32902\n",
      "Epoch 600/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.1472 - val_loss: 0.9352\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.32902\n",
      "Epoch 601/750000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.8584 - val_loss: 0.9188\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.32902\n",
      "Epoch 602/750000\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.9344 - val_loss: 0.9004\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.32902\n",
      "Epoch 603/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 435ms/step - loss: 0.7743 - val_loss: 0.8764\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.32902\n",
      "Epoch 604/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 1.0587 - val_loss: 0.8093\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.32902\n",
      "Epoch 605/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.9547 - val_loss: 0.7299\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.32902\n",
      "Epoch 606/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.6665 - val_loss: 0.7144\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.32902\n",
      "Epoch 607/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.5894 - val_loss: 0.5961\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.32902\n",
      "Epoch 608/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1899 - val_loss: 0.5786\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.32902\n",
      "Epoch 609/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.5378 - val_loss: 0.5556\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.32902\n",
      "Epoch 610/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.4093 - val_loss: 0.6219\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.32902\n",
      "Epoch 611/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.8128 - val_loss: 0.7158\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.32902\n",
      "Epoch 612/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.7834 - val_loss: 0.8826\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.32902\n",
      "Epoch 613/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8402 - val_loss: 1.0004\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.32902\n",
      "Epoch 614/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7838 - val_loss: 1.0938\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.32902\n",
      "Epoch 615/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 1.1144 - val_loss: 1.2666\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.32902\n",
      "Epoch 616/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.7458 - val_loss: 1.2884\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.32902\n",
      "Epoch 617/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.0024 - val_loss: 1.3862\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.32902\n",
      "Epoch 618/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.7348 - val_loss: 1.4117\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.32902\n",
      "Epoch 619/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.8973 - val_loss: 1.4184\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.32902\n",
      "Epoch 620/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.7890 - val_loss: 1.3978\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.32902\n",
      "Epoch 621/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.2463 - val_loss: 1.4039\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.32902\n",
      "Epoch 622/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.7144 - val_loss: 1.4615\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.32902\n",
      "Epoch 623/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.9199 - val_loss: 1.6169\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.32902\n",
      "Epoch 624/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.0485 - val_loss: 1.6684\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.32902\n",
      "Epoch 625/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.4504 - val_loss: 1.6624\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.32902\n",
      "Epoch 626/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.5002 - val_loss: 1.6238\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.32902\n",
      "Epoch 627/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.0176 - val_loss: 1.5666\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.32902\n",
      "Epoch 628/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.2903 - val_loss: 1.3835\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.32902\n",
      "Epoch 629/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.0473 - val_loss: 1.2653\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.32902\n",
      "Epoch 630/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.1782 - val_loss: 1.2158\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.32902\n",
      "Epoch 631/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.8823 - val_loss: 1.2025\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.32902\n",
      "Epoch 632/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.4706 - val_loss: 1.2199\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.32902\n",
      "Epoch 633/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8729 - val_loss: 1.2926\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.32902\n",
      "Epoch 634/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1894 - val_loss: 1.2902\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.32902\n",
      "Epoch 635/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.0397 - val_loss: 1.3785\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.32902\n",
      "Epoch 636/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.8927 - val_loss: 1.4687\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.32902\n",
      "Epoch 637/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9967 - val_loss: 1.6035\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.32902\n",
      "Epoch 638/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1983 - val_loss: 1.6993\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.32902\n",
      "Epoch 639/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.8486 - val_loss: 1.7301\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.32902\n",
      "Epoch 640/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9055 - val_loss: 1.6607\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.32902\n",
      "Epoch 641/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.5776 - val_loss: 1.5672\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.32902\n",
      "Epoch 642/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1542 - val_loss: 1.3750\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.32902\n",
      "Epoch 643/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.6695 - val_loss: 1.3155\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.32902\n",
      "Epoch 644/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.9322 - val_loss: 1.2032\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.32902\n",
      "Epoch 645/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.8301 - val_loss: 1.1751\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.32902\n",
      "Epoch 646/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.8463 - val_loss: 1.1182\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.32902\n",
      "Epoch 647/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 1.3058 - val_loss: 1.0296\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.32902\n",
      "Epoch 648/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.6321 - val_loss: 0.9964\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.32902\n",
      "Epoch 649/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8320 - val_loss: 0.9658\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.32902\n",
      "Epoch 650/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.0872 - val_loss: 0.8310\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.32902\n",
      "Epoch 651/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.9250 - val_loss: 0.7743\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.32902\n",
      "Epoch 652/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.9861 - val_loss: 0.7193\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.32902\n",
      "Epoch 653/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.0320 - val_loss: 0.7715\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.32902\n",
      "Epoch 654/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.0028 - val_loss: 0.7300\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.32902\n",
      "Epoch 655/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 1.0625 - val_loss: 0.6913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00655: val_loss did not improve from 0.32902\n",
      "Epoch 656/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.9308 - val_loss: 0.6314\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.32902\n",
      "Epoch 657/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.6463 - val_loss: 0.5119\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.32902\n",
      "Epoch 658/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.7815 - val_loss: 0.4156\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.32902\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.416.hdf5\n",
      "Epoch 659/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.6928 - val_loss: 0.3085\n",
      "\n",
      "Epoch 00659: val_loss improved from 0.32902 to 0.30850, saving model to ./weights/weights_tail_K/DenseNet201.hdf5\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.308.hdf5\n",
      "Epoch 660/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.0852 - val_loss: 0.3031\n",
      "\n",
      "Epoch 00660: val_loss improved from 0.30850 to 0.30305, saving model to ./weights/weights_tail_K/DenseNet201.hdf5\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.303.hdf5\n",
      "Epoch 661/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 1.0123 - val_loss: 0.3126\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.313.hdf5\n",
      "Epoch 662/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.2216 - val_loss: 0.3386\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.339.hdf5\n",
      "Epoch 663/750000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 1.1062 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.356.hdf5\n",
      "Epoch 664/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.5083 - val_loss: 0.3721\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.372.hdf5\n",
      "Epoch 665/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 1.2577 - val_loss: 0.3701\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.37.hdf5\n",
      "Epoch 666/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.7896 - val_loss: 0.4069\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.407.hdf5\n",
      "Epoch 667/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 1.1144 - val_loss: 0.4389\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.439.hdf5\n",
      "Epoch 668/750000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 1.1682 - val_loss: 0.5265\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.30305\n",
      "Epoch 669/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 1.5835 - val_loss: 0.5326\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.30305\n",
      "Epoch 670/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.1636 - val_loss: 0.4045\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.404.hdf5\n",
      "Epoch 671/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.7980 - val_loss: 0.3078\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.308.hdf5\n",
      "Epoch 672/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.7814 - val_loss: 0.3256\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.326.hdf5\n",
      "Epoch 673/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.6408 - val_loss: 0.3443\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.344.hdf5\n",
      "Epoch 674/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.9344 - val_loss: 0.3906\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.391.hdf5\n",
      "Epoch 675/750000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.9993 - val_loss: 0.4089\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.409.hdf5\n",
      "Epoch 676/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.6755 - val_loss: 0.4886\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.30305\n",
      "Epoch 677/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.6798 - val_loss: 0.5162\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.30305\n",
      "Epoch 678/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8718 - val_loss: 0.5506\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.30305\n",
      "Epoch 679/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8121 - val_loss: 0.5237\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.30305\n",
      "Epoch 680/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.7753 - val_loss: 0.5461\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.30305\n",
      "Epoch 681/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.8364 - val_loss: 0.5044\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.30305\n",
      "Epoch 682/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0804 - val_loss: 0.5567\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.30305\n",
      "Epoch 683/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.4026 - val_loss: 0.7119\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.30305\n",
      "Epoch 684/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.5729 - val_loss: 0.9265\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.30305\n",
      "Epoch 685/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.1279 - val_loss: 1.2657\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.30305\n",
      "Epoch 686/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.4014 - val_loss: 1.4001\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.30305\n",
      "Epoch 687/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.3523 - val_loss: 1.4669\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.30305\n",
      "Epoch 688/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7665 - val_loss: 1.3595\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.30305\n",
      "Epoch 689/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.1564 - val_loss: 1.3543\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.30305\n",
      "Epoch 690/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.6496 - val_loss: 1.3353\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.30305\n",
      "Epoch 691/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9341 - val_loss: 1.5153\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.30305\n",
      "Epoch 692/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3191 - val_loss: 1.5945\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.30305\n",
      "Epoch 693/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1643 - val_loss: 1.7971\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.30305\n",
      "Epoch 694/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.6105 - val_loss: 1.6605\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.30305\n",
      "Epoch 695/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.7590 - val_loss: 1.7019\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.30305\n",
      "Epoch 696/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7682 - val_loss: 1.6994\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.30305\n",
      "Epoch 697/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0176 - val_loss: 1.7876\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.30305\n",
      "Epoch 698/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 1.2181 - val_loss: 1.7052\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.30305\n",
      "Epoch 699/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7426 - val_loss: 1.7711\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.30305\n",
      "Epoch 700/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.7957 - val_loss: 1.7635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00700: val_loss did not improve from 0.30305\n",
      "Epoch 701/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8963 - val_loss: 1.8971\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.30305\n",
      "Epoch 702/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9817 - val_loss: 1.7968\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.30305\n",
      "Epoch 703/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.2757 - val_loss: 1.8091\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.30305\n",
      "Epoch 704/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.6863 - val_loss: 1.6579\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.30305\n",
      "Epoch 705/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.1845 - val_loss: 1.4610\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.30305\n",
      "Epoch 706/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.3663 - val_loss: 1.1712\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.30305\n",
      "Epoch 707/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8663 - val_loss: 1.0253\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.30305\n",
      "Epoch 708/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.8076 - val_loss: 0.9353\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.30305\n",
      "Epoch 709/750000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.9882 - val_loss: 0.8813\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.30305\n",
      "Epoch 710/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1134 - val_loss: 0.8382\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.30305\n",
      "Epoch 711/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.3086 - val_loss: 0.8436\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.30305\n",
      "Epoch 712/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.9684 - val_loss: 0.8582\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.30305\n",
      "Epoch 713/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 1.0917 - val_loss: 0.8998\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.30305\n",
      "Epoch 714/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.8515 - val_loss: 0.9305\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.30305\n",
      "Epoch 715/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.2760 - val_loss: 0.9829\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.30305\n",
      "Epoch 716/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.8848 - val_loss: 0.9777\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.30305\n",
      "Epoch 717/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.9357 - val_loss: 1.0837\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.30305\n",
      "Epoch 718/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 1.4824 - val_loss: 1.0478\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.30305\n",
      "Epoch 719/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8794 - val_loss: 0.9679\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.30305\n",
      "Epoch 720/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1653 - val_loss: 0.7813\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.30305\n",
      "Epoch 721/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8164 - val_loss: 0.6344\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.30305\n",
      "Epoch 722/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.7506 - val_loss: 0.5431\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.30305\n",
      "Epoch 723/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.2621 - val_loss: 0.3690\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.369.hdf5\n",
      "Epoch 724/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.5650 - val_loss: 0.3479\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.348.hdf5\n",
      "Epoch 725/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2225 - val_loss: 0.3113\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.311.hdf5\n",
      "Epoch 726/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.8857 - val_loss: 0.3440\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.344.hdf5\n",
      "Epoch 727/750000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.8919 - val_loss: 0.3487\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.349.hdf5\n",
      "Epoch 728/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.4072 - val_loss: 0.3575\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.357.hdf5\n",
      "Epoch 729/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.8529 - val_loss: 0.3676\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.368.hdf5\n",
      "Epoch 730/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.1092 - val_loss: 0.3992\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.399.hdf5\n",
      "Epoch 731/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 1.5037 - val_loss: 0.4128\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.413.hdf5\n",
      "Epoch 732/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.4794 - val_loss: 0.4353\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.30305\n",
      "Saving to weights\\weights_tail_K\\DenseNet201_tail_K_MAE0.435.hdf5\n",
      "Epoch 733/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 1.1627 - val_loss: 0.5265\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.30305\n",
      "Epoch 734/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.6727 - val_loss: 0.5391\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.30305\n",
      "Epoch 735/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9165 - val_loss: 0.5413\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.30305\n",
      "Epoch 736/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.3986 - val_loss: 0.5007\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.30305\n",
      "Epoch 737/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.5444 - val_loss: 0.5618\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.30305\n",
      "Epoch 738/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.7277 - val_loss: 0.5629\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.30305\n",
      "Epoch 739/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.8576 - val_loss: 0.6018\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.30305\n",
      "Epoch 740/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.3142 - val_loss: 0.5271\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.30305\n",
      "Epoch 741/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.5773 - val_loss: 0.5050\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.30305\n",
      "Epoch 742/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.9088 - val_loss: 0.4737\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.30305\n",
      "Epoch 743/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.4468 - val_loss: 0.5006\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.30305\n",
      "Epoch 744/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.9804 - val_loss: 0.5241\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.30305\n",
      "Epoch 745/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.7093 - val_loss: 0.5330\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.30305\n",
      "Epoch 746/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.9445 - val_loss: 0.5226\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.30305\n",
      "Epoch 747/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8514 - val_loss: 0.5054\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.30305\n",
      "Epoch 748/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.7208 - val_loss: 0.4997\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.30305\n",
      "Epoch 749/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 414ms/step - loss: 1.1234 - val_loss: 0.5072\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.30305\n",
      "Epoch 750/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.1088 - val_loss: 0.5408\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.30305\n",
      "Epoch 751/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9230 - val_loss: 0.5260\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.30305\n",
      "Epoch 752/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.5400 - val_loss: 0.5087\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.30305\n",
      "Epoch 753/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.6962 - val_loss: 0.4961\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.30305\n",
      "Epoch 754/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9626 - val_loss: 0.4972\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.30305\n",
      "Epoch 755/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9892 - val_loss: 0.4998\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.30305\n",
      "Epoch 756/750000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.6060 - val_loss: 0.4948\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.30305\n",
      "Epoch 757/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9257 - val_loss: 0.5135\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.30305\n",
      "Epoch 758/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.6066 - val_loss: 0.5700\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.30305\n",
      "Epoch 759/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2848 - val_loss: 0.5604\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.30305\n",
      "Epoch 760/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.9009 - val_loss: 0.5681\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.30305\n",
      "Epoch 761/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.9356 - val_loss: 0.5170\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.30305\n",
      "Epoch 762/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9701 - val_loss: 0.5126\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.30305\n",
      "Epoch 763/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.6527 - val_loss: 0.4863\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.30305\n",
      "Epoch 764/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.8809 - val_loss: 0.4951\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.30305\n",
      "Epoch 765/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.3091 - val_loss: 0.4860\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.30305\n",
      "Epoch 766/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.1505 - val_loss: 0.5394\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.30305\n",
      "Epoch 767/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.6243 - val_loss: 0.5763\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.30305\n",
      "Epoch 768/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.0281 - val_loss: 0.5870\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.30305\n",
      "Epoch 769/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.7564 - val_loss: 0.5452\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.30305\n",
      "Epoch 770/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.7217 - val_loss: 0.5121\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.30305\n",
      "Epoch 771/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.6560 - val_loss: 0.4707\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.30305\n",
      "Epoch 772/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0446 - val_loss: 0.4715\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.30305\n",
      "Epoch 773/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.8227 - val_loss: 0.4754\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.30305\n",
      "Epoch 774/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.8275 - val_loss: 0.4789\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.30305\n",
      "Epoch 775/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 1.0261 - val_loss: 0.5038\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.30305\n",
      "Epoch 776/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 1.2461 - val_loss: 0.4969\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.30305\n",
      "Epoch 777/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.9928 - val_loss: 0.5135\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.30305\n",
      "Epoch 778/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.0186 - val_loss: 0.5394\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.30305\n",
      "Epoch 779/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.0208 - val_loss: 0.6210\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.30305\n",
      "Epoch 780/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.1645 - val_loss: 0.5789\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.30305\n",
      "Epoch 781/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.1040 - val_loss: 0.5150\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.30305\n",
      "Epoch 782/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8568 - val_loss: 0.5498\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.30305\n",
      "Epoch 783/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.9548 - val_loss: 0.6560\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.30305\n",
      "Epoch 784/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9891 - val_loss: 0.8388\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.30305\n",
      "Epoch 785/750000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 1.4987 - val_loss: 0.8331\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.30305\n",
      "Epoch 786/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.9245 - val_loss: 0.7974\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.30305\n",
      "Epoch 787/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0030 - val_loss: 0.6359\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.30305\n",
      "Epoch 788/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8060 - val_loss: 0.5651\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.30305\n",
      "Epoch 789/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.6654 - val_loss: 0.5615\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.30305\n",
      "Epoch 790/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.8529 - val_loss: 0.5655\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.30305\n",
      "Epoch 791/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.5708 - val_loss: 0.5736\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.30305\n",
      "Epoch 792/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.0226 - val_loss: 0.5628\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.30305\n",
      "Epoch 793/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.2666 - val_loss: 0.5830\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.30305\n",
      "Epoch 794/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.9432 - val_loss: 0.5775\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.30305\n",
      "Epoch 795/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.8351 - val_loss: 0.6411\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.30305\n",
      "Epoch 796/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.7307 - val_loss: 0.7018\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.30305\n",
      "Epoch 797/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0843 - val_loss: 0.7342\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.30305\n",
      "Epoch 798/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.3563 - val_loss: 0.7720\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.30305\n",
      "Epoch 799/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.5074 - val_loss: 0.8035\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.30305\n",
      "Epoch 800/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.5897 - val_loss: 0.8430\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.30305\n",
      "Epoch 801/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.3692 - val_loss: 0.8165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00801: val_loss did not improve from 0.30305\n",
      "Epoch 802/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.2147 - val_loss: 0.7919\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.30305\n",
      "Epoch 803/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.4517 - val_loss: 0.8025\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.30305\n",
      "Epoch 804/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4550 - val_loss: 0.7638\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.30305\n",
      "Epoch 805/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.7097 - val_loss: 0.7481\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.30305\n",
      "Epoch 806/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 1.2929 - val_loss: 0.7094\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.30305\n",
      "Epoch 807/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 1.3226 - val_loss: 0.7087\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.30305\n",
      "Epoch 808/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9502 - val_loss: 0.7349\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.30305\n",
      "Epoch 809/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.4877 - val_loss: 0.7865\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.30305\n",
      "Epoch 810/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.1324 - val_loss: 0.8166\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.30305\n",
      "Epoch 811/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.6837 - val_loss: 0.8561\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.30305\n",
      "Epoch 812/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.5656 - val_loss: 0.9665\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.30305\n",
      "Epoch 813/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.1417 - val_loss: 1.0589\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.30305\n",
      "Epoch 814/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.5440 - val_loss: 1.1019\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.30305\n",
      "Epoch 815/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2243 - val_loss: 0.9887\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.30305\n",
      "Epoch 816/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0931 - val_loss: 0.9230\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.30305\n",
      "Epoch 817/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.5240 - val_loss: 0.8423\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.30305\n",
      "Epoch 818/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.7352 - val_loss: 0.7821\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.30305\n",
      "Epoch 819/750000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 1.2930 - val_loss: 0.7199\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.30305\n",
      "Epoch 820/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.9519 - val_loss: 0.6135\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.30305\n",
      "Epoch 821/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.0997 - val_loss: 0.5934\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.30305\n",
      "Epoch 822/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.1778 - val_loss: 0.5508\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.30305\n",
      "Epoch 823/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 1.4230 - val_loss: 0.5722\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.30305\n",
      "Epoch 824/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.0346 - val_loss: 0.5683\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.30305\n",
      "Epoch 825/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.8073 - val_loss: 0.5959\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.30305\n",
      "Epoch 826/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.9962 - val_loss: 0.6418\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.30305\n",
      "Epoch 827/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.5716 - val_loss: 0.6618\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.30305\n",
      "Epoch 828/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.8754 - val_loss: 0.6551\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.30305\n",
      "Epoch 829/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0816 - val_loss: 0.6241\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.30305\n",
      "Epoch 830/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.2939 - val_loss: 0.6155\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.30305\n",
      "Epoch 831/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.9679 - val_loss: 0.6149\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.30305\n",
      "Epoch 832/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 1.2142 - val_loss: 0.5987\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.30305\n",
      "Epoch 833/750000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.9691 - val_loss: 0.5672\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.30305\n",
      "Epoch 834/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.8391 - val_loss: 0.5384\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.30305\n",
      "Epoch 835/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.1410 - val_loss: 0.5415\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.30305\n",
      "Epoch 836/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 1.0973 - val_loss: 0.5536\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.30305\n",
      "Epoch 837/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.6715 - val_loss: 0.5460\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.30305\n",
      "Epoch 838/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.7419 - val_loss: 0.6067\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.30305\n",
      "Epoch 839/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.5540 - val_loss: 0.6498\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.30305\n",
      "Epoch 840/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.5561 - val_loss: 0.6936\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.30305\n",
      "Epoch 841/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.8904 - val_loss: 0.6732\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.30305\n",
      "Epoch 842/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1810 - val_loss: 0.6701\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.30305\n",
      "Epoch 843/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.5907 - val_loss: 0.6747\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.30305\n",
      "Epoch 844/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.8878 - val_loss: 0.6823\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.30305\n",
      "Epoch 845/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.1081 - val_loss: 0.7221\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.30305\n",
      "Epoch 846/750000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 1.4493 - val_loss: 0.7119\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.30305\n",
      "Epoch 847/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.7097 - val_loss: 0.7072\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.30305\n",
      "Epoch 848/750000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 1.0120 - val_loss: 0.7470\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.30305\n",
      "Epoch 849/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.1094 - val_loss: 0.9477\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.30305\n",
      "Epoch 850/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.5991 - val_loss: 1.1566\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.30305\n",
      "Epoch 851/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.4490 - val_loss: 1.2345\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.30305\n",
      "Epoch 852/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.5943 - val_loss: 1.2867\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.30305\n",
      "Epoch 853/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0294 - val_loss: 1.3823\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.30305\n",
      "Epoch 854/750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 420ms/step - loss: 0.9661 - val_loss: 1.2239\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.30305\n",
      "Epoch 855/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.0736 - val_loss: 1.0460\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.30305\n",
      "Epoch 856/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.1854 - val_loss: 0.9046\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.30305\n",
      "Epoch 857/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.1084 - val_loss: 0.8745\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.30305\n",
      "Epoch 858/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.7100 - val_loss: 0.8763\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.30305\n",
      "Epoch 859/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.4650 - val_loss: 0.9809\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.30305\n",
      "Epoch 860/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 1.1198 - val_loss: 1.0385\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.30305\n",
      "Epoch 861/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.9758 - val_loss: 1.1131\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.30305\n",
      "Epoch 862/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 1.0459 - val_loss: 1.0976\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.30305\n",
      "Epoch 863/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9665 - val_loss: 1.0416\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.30305\n",
      "Epoch 864/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.0992 - val_loss: 0.9242\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.30305\n",
      "Epoch 865/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.1188 - val_loss: 0.8591\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.30305\n",
      "Epoch 866/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 1.2775 - val_loss: 0.7965\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.30305\n",
      "Epoch 867/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.0325 - val_loss: 0.7075\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.30305\n",
      "Epoch 868/750000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.9073 - val_loss: 0.6678\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.30305\n",
      "Epoch 869/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.7798 - val_loss: 0.6503\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.30305\n",
      "Epoch 870/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.0655 - val_loss: 0.7070\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.30305\n",
      "Epoch 871/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.6214 - val_loss: 0.7345\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.30305\n",
      "Epoch 872/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.8808 - val_loss: 0.8942\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.30305\n",
      "Epoch 873/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.9031 - val_loss: 0.8094\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.30305\n",
      "Epoch 874/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 1.1426 - val_loss: 0.7372\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.30305\n",
      "Epoch 875/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.9478 - val_loss: 0.6924\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.30305\n",
      "Epoch 876/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.8964 - val_loss: 0.6984\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.30305\n",
      "Epoch 877/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.8160 - val_loss: 0.7437\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.30305\n",
      "Epoch 878/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1997 - val_loss: 0.8577\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.30305\n",
      "Epoch 879/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.8837 - val_loss: 0.9159\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.30305\n",
      "Epoch 880/750000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.7495 - val_loss: 1.1155\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.30305\n",
      "Epoch 881/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.3277 - val_loss: 1.1250\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.30305\n",
      "Epoch 882/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.8705 - val_loss: 1.2000\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.30305\n",
      "Epoch 883/750000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.5498 - val_loss: 1.0627\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.30305\n",
      "Epoch 884/750000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.8750 - val_loss: 1.0830\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.30305\n",
      "Epoch 885/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.2287 - val_loss: 0.9794\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.30305\n",
      "Epoch 886/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.4540 - val_loss: 0.9964\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.30305\n",
      "Epoch 887/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.9945 - val_loss: 0.9722\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.30305\n",
      "Epoch 888/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.9417 - val_loss: 0.9040\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.30305\n",
      "Epoch 889/750000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 1.1080 - val_loss: 0.7839\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.30305\n",
      "Epoch 890/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.7979 - val_loss: 0.7392\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.30305\n",
      "Epoch 891/750000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 1.0479 - val_loss: 0.6900\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.30305\n",
      "Epoch 892/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.0350 - val_loss: 0.6904\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.30305\n",
      "Epoch 893/750000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 1.0430 - val_loss: 0.6689\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.30305\n",
      "Epoch 894/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.8190 - val_loss: 0.6830\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.30305\n",
      "Epoch 895/750000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 1.1827 - val_loss: 0.6862\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.30305\n",
      "Epoch 896/750000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 1.2134 - val_loss: 0.7158\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.30305\n",
      "Epoch 897/750000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 1.2105 - val_loss: 0.7408\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.30305\n",
      "Epoch 898/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.5235 - val_loss: 0.8356\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.30305\n",
      "Epoch 899/750000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.8488 - val_loss: 0.9472\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.30305\n",
      "Epoch 900/750000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 1.0408 - val_loss: 1.1223\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.30305\n",
      "Epoch 901/750000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 1.1475 - val_loss: 1.1793\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.30305\n",
      "Epoch 902/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.8899 - val_loss: 1.3095\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.30305\n",
      "Epoch 903/750000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 1.1779 - val_loss: 1.4058\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.30305\n",
      "Epoch 904/750000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 1.7029 - val_loss: 1.4726\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.30305\n",
      "Epoch 905/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.8918 - val_loss: 1.4176\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.30305\n",
      "Epoch 906/750000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 1.0804 - val_loss: 1.4672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00906: val_loss did not improve from 0.30305\n",
      "Epoch 907/750000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.9642 - val_loss: 1.3718\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.30305\n",
      "Epoch 908/750000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 1.5369 - val_loss: 1.2896\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.30305\n",
      "Epoch 909/750000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 1.6273 - val_loss: 1.0482\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.30305\n",
      "Epoch 910/750000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.8419 - val_loss: 0.9105\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.30305\n",
      "Epoch 911/750000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6563"
     ]
    }
   ],
   "source": [
    "batch_size_train = 4\n",
    "epochs_per_iteration = 250000\n",
    "batch_size_validate = len(validate_labels) // 4\n",
    "\n",
    "save_dir_weights = './weights/weights_' + with_K + '_K/'\n",
    "if not os.path.exists(save_dir_weights):\n",
    "    os.mkdir(save_dir_weights)\n",
    "\n",
    "checkpointer_best = ModelCheckpoint(\n",
    "    filepath=save_dir_weights + net + '.hdf5',\n",
    "    monitor='val_loss', verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "saver_MAE = SaveModelOnMAE_tail()\n",
    "print('Training ...')\n",
    "train_generator = generate_generator(train_paths, train_labels, train_K, batch_size_train, net=net, with_K=with_K, K_len=K_len)\n",
    "validate_generator = generate_generator(validate_paths, validate_labels, validate_K, batch_size_validate, net=net, with_K=with_K, K_len=K_len)\n",
    "history = model.fit_generator(\n",
    "    train_generator, steps_per_epoch = np.ceil(len(train_paths)/batch_size_train/epochs_per_iteration),\n",
    "    epochs=epochs_per_iteration * 3,\n",
    "    validation_data=validate_generator, validation_steps=2,\n",
    "    verbose=1, callbacks=[checkpointer_best, saver_MAE]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl, l = history.history['val_loss'], history.history['loss']\n",
    "vl, l = np.asarray(vl), np.asarray(l)\n",
    "vl[vl > 1], l[l > 1] = 1, 1\n",
    "\n",
    "plt.plot(vl, 'r')\n",
    "plt.plot(l, 'b')\n",
    "plt.legend(['val_loss', 'loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1/7 model: ./weights/weights_tail_K/DenseNet201_tail_K_MAE0.303.hdf5\n",
      "(195,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "K_len = 1920\n",
    "losses = [0.303, 0.308, 0.311, 0.313, 0.326, 0.329, 0.335]\n",
    "for idx, loss in enumerate(losses[:1]):\n",
    "    print('Loading {}/{} model: {}'.format(idx+1, len(losses), './weights/weights_tail_K/DenseNet201_tail_K_MAE'+str(loss)+'.hdf5'))\n",
    "    model = load_model('./weights/weights_tail_K/DenseNet201_tail_K_MAE'+str(loss)+'.hdf5')\n",
    "    generator_for_test = generate_generator(test_paths_48, test_labels_48, test_K_48, len(test_labels_48), net=net, with_K=with_K, K_len=K_len)\n",
    "    X_test, Y_test = generator_for_test.__next__()\n",
    "    preds = np.squeeze(model.predict(X_test))\n",
    "    fg, (ax0, ax1) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    ax0.plot(Y_test, 'b')\n",
    "    ax0.plot(preds, 'r')\n",
    "    ax0.legend(['real_values', 'preds'])\n",
    "    ax1.plot(np.abs(preds - Y_test))\n",
    "    MAE = round(np.mean(abs(preds - Y_test)), 3)\n",
    "    ax1.set_title('On test_48 set, MAE = {}'.format(MAE))\n",
    "    print(np.unique(preds).shape)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "label_pred = np.hstack([np.asarray(range(Y_test.shape[0])).reshape(-1, 1), Y_test.reshape(-1, 1), preds.reshape(-1, 1)])\n",
    "label_pred = pd.DataFrame(label_pred)\n",
    "label_pred.to_csv(\n",
    "    os.path.join('preds', 'preds_' + with_K + '_K', '_'.join([net, with_K, 'K_MAE', str(MAE) + '.txt'])),\n",
    "    index=None,\n",
    "    header=['index', 'label', 'prediction']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
